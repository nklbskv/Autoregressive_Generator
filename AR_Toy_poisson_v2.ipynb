{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3233e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb39d1",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f286ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CONDITION = False          # <- auf False setzen für unkonditioniert\n",
    "d_model = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "d_ff = 512\n",
    "dropout = 0.2\n",
    "batch_size = 1024\n",
    "eval_iters = 500\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31e5bd",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n",
    "Schritte:\n",
    "1. Jeder wert kriegt ein eigenes lambda\n",
    "2. füge den wert n_lambda mal einem string hinzu\n",
    "3. shuffle die sequenz\n",
    "4. erstelle solche strings N mal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "393790bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(1, 5)\n",
    "rates = {1: 6.0, 2: 3.0, 3: 1.5, 4: 0.7}\n",
    "np.random.seed(42)\n",
    "\n",
    "# einzelne sequenz mit poissonverteilten werten innerhalb der sequenz\n",
    "def generate_poisson_string(values):\n",
    "    count = 0\n",
    "    seq = []\n",
    "    for val in values:\n",
    "        count = np.random.poisson(rates[val])\n",
    "        for _ in range(count):\n",
    "            seq.append(val)\n",
    "        count=0\n",
    "    seq = np.array(seq)\n",
    "    np.random.shuffle(seq)\n",
    "    return seq\n",
    "\n",
    "# hänge N sequenzen aneinander und fülle immer bis zur maximalen länge mit 0 auf\n",
    "def generate_dataset(N):\n",
    "    sequences = [generate_poisson_string(values) for _ in range(N)]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = np.array([\n",
    "    np.pad(seq, (0, max_len - len(seq)), mode='constant', constant_values=0) for seq in sequences])\n",
    "    return padded_sequences\n",
    "\n",
    "data = generate_dataset(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0703062",
   "metadata": {},
   "source": [
    "# Make list of sequences as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7325db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_lol(dataset):\n",
    "    seqs = []\n",
    "    for row in dataset:\n",
    "        zero_idx = np.argmax(row == 0) if np.any(row == 0) else len(row)\n",
    "        seqs.append(row[:zero_idx])\n",
    "\n",
    "    return seqs\n",
    "\n",
    "seqs = dataset_to_lol(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbee42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vokabular der Ausgabewerte\n",
    "vals = sorted({v for s in seqs for v in s})\n",
    "\n",
    "# Spezielle Tokens\n",
    "PAD_ID = 0 \n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "offset = 3  # Start-ID für echte Werte\n",
    "stoi = {v: i+offset for i,v in enumerate(vals)}\n",
    "itos = {i+offset: v for i,v in enumerate(vals)}\n",
    "\n",
    "vocab_size = offset + len(vals) \n",
    "max_len = max(len(s) for s in seqs)\n",
    "\n",
    "def encode_example(seq):\n",
    "    # Tokenize: BOS + seq + EOS, dann pad\n",
    "    toks = [BOS_ID]\n",
    "    toks += [stoi[v] for v in seq]\n",
    "    toks.append(EOS_ID)\n",
    "    # Ziel ist um 1 nach rechts geschoben\n",
    "    attn_len = len(toks)\n",
    "    pad_needed = ( (max_len + 2) - attn_len )\n",
    "    toks += [PAD_ID] * pad_needed\n",
    "    x = torch.tensor(toks[:-1], dtype=torch.long)  # inputs\n",
    "    y = torch.tensor(toks[1:],  dtype=torch.long)  # targets\n",
    "    return x, y\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, seqs):\n",
    "        self.items = [encode_example(s) for s in seqs]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "dataset = SeqDataset(seqs)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  \n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ee5a6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2731d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_head, n_layer, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len + 1, d_model))  # +1 für BOS\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                                    nhead=n_head, \n",
    "                                                    dim_feedforward=d_ff, \n",
    "                                                    dropout=dropout,\n",
    "                                                    batch_first=True,\n",
    "                                                    activation='gelu').to(device)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layer, enable_nested_tensor=False).to(device)\n",
    "        self.ff = nn.Linear(d_model, vocab_size)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.size() #Batch, num sequences/time\n",
    "        h = self.token_emb(x) + self.pos_emb[:, :T, :]\n",
    "        causal_mask = torch.tril(torch.ones(T, T)).to(x.device).bool()\n",
    "        key_padding_mask = (x == PAD_ID)\n",
    "        h = self.transformer(h, mask=causal_mask, src_key_padding_mask=key_padding_mask)\n",
    "        h = self.ln(h)\n",
    "        logits = self.ff(h)\n",
    "            \n",
    "        if targets is not None:\n",
    "            B,T,C = logits.size()\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=PAD_ID)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            loss=None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b7bbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalTransformer(vocab_size, d_model, n_head, n_layer, d_ff, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afeaed",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "372229fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, eval_iters=500):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = []\n",
    "        for i, (xb, yb) in enumerate(loader):\n",
    "            if eval_iters is not None and i >= eval_iters:\n",
    "                break\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses) if losses else float('inf')\n",
    "    model.train()\n",
    "    return out\n",
    "            \n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, eval_iters=50, lr=1e-3):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    #Per epoch losses\n",
    "    train_losses = []\n",
    "    val_losses =  []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        metrics = estimate_loss(model, train_loader, val_loader, eval_iters=eval_iters)\n",
    "        val_losses.append(metrics['val'])\n",
    "        train_losses.append(metrics['train'])\n",
    "\n",
    "        if epoch == epochs: break\n",
    "    \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in tqdm(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            _, loss = model(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73854744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3773f8315d2469f89a277a3e7eab0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba254030b30445978d0db3a10b6d1584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe5a2c24acf4de49b3cda1cfabccf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c327534f38e44aaf96803127050950ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455010146e4f43cba5f71d31bbbe65b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c700a37fbe4345bdbbbcf5c975a7e7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, epochs=5, eval_iters=eval_iters, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2528aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 0.025401572620434444,\n",
       " nan,\n",
       " 0.0005067086936080937,\n",
       " nan,\n",
       " 0.0004050570568209186,\n",
       " nan,\n",
       " 0.0003816378037872675,\n",
       " nan,\n",
       " 0.00043399887130055476]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses, val_losses = trained_model\n",
    "train_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
